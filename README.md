# Embedding, Clustering and Vectorization Workshop

## Dataset Description

### 1. Brown Corpus (NLTK)
We use the Brown Corpus (news category) as our main corpus for training Word2Vec models.  
It provides balanced English text samples and is publicly available through NLTK.
- Source: https://www.nltk.org/nltk_data/
- License: Public domain (via NLTK)
### 2. Pretrained GloVe Embeddings (50d)
For the GloVe (count-based) embedding experiment, we use the **GloVe 6B 50-dimensional pretrained vectors**.  

**Important:**  
The file `glove.6B.50d.txt` is **large (171MB)** and **cannot be uploaded to GitHub** due to size limits.  
You must **download it manually** and place it in your project folder before running the notebook.
- Download link: https://nlp.stanford.edu/data/glove.6B.zip
- Extract the file `glove.6B.50d.txt` into ./glove.6B.50d/glove.6B.50d.txt

## Team Members
- Enuie Jo
- Kihoon Kim

## Dataset Links
- Brown Corpus (NLTK): https://www.nltk.org/nltk_data/
- GloVe 6B dataset: https://nlp.stanford.edu/data/glove.6B.zip

